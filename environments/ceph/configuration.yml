---
##########################
# generic

containerized_deployment: true

osd_objectstore: bluestore
osd_scenario: lvm

generate_fsid: false
fsid: c2120a4a-669c-4769-a32c-b7e9d7b848f4

##########################
# osd

dmcrypt: false
# NOTE: It is common to place more than 1 OSD on flash devices. To simulate
#       upgrades etc. with this approach this is also used here.
osds_per_device: 1
crush_rule_config: true

##########################
# network

public_network: 192.168.16.0/20
cluster_network: 192.168.16.0/20

# OpenStack pool config moved to inventory/group_vars/ceph.yml

openstack_keys:
  - name: client.cinder-backup
    caps:
      mon: "allow r"
      osd: "allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_cinder_backup_pool.name }}"
    mode: "0600"
  - name: client.cinder
    caps:
      mon: "allow r"
      osd: "allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_cinder_pool.name }}, allow rwx pool={{ openstack_nova_pool.name }}, allow rx pool={{ openstack_glance_pool.name }}"
    mode: "0600"
  - name: client.glance
    caps:
      mon: "allow r"
      osd: "allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_glance_pool.name }}"
    mode: "0600"
  - name: client.gnocchi
    caps:
      mon: "allow r"
      osd: "allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_gnocchi_pool.name }}"
    mode: "0600"
  - name: client.nova
    caps:
      mon: "allow r"
      osd: "allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_glance_pool.name }}, allow rwx pool={{ openstack_nova_pool.name }}, allow rwx pool={{ openstack_cinder_pool.name }}, allow rwx pool={{ openstack_cinder_backup_pool.name }}"
    mode: "0600"

##########################
# manager

ceph_mgr_modules:
  - balancer
  - dashboard
  - prometheus
  - status



##########################
# custom

ceph_conf_overrides:
  global:
    mon host: 192.168.16.10
    osd pool default pg num: 8
    osd pool default size: 3
    osd pool default min size: 2
    auth allow insecure global id reclaim: false

  mon:
    mon allow pool delete: true
    mon max pg per osd: 400

  "client.rgw.{{ hostvars[inventory_hostname]['ansible_hostname'] }}.rgw0":
    "rgw content length compat": "true"
    "rgw enable apis": "swift, s3, admin"
    "rgw keystone accepted roles": "member, admin"
    "rgw keystone accepted admin roles": "admin"
    "rgw keystone admin domain": "default"
    "rgw keystone admin password": "{{ swift_keystone_password }}"
    "rgw keystone admin project": "service"
    "rgw keystone admin tenant": "service"
    "rgw keystone admin user": "swift"
    "rgw keystone api version": "3"
    "rgw keystone url": "https://api.in-a-box.cloud"
    "rgw keystone implicit tenants": "true"
    "rgw s3 auth use keystone": "true"
    "rgw swift account in url": "true"
    "rgw swift versioning enabled": "true"

# CephFS and RGW pool config moved to inventory/group_vars/ceph.yml
