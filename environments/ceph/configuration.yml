---
##########################
# generic

fsid: c2120a4a-669c-4769-a32c-b7e9d7b848f4

##########################
# osd

dmcrypt: false
# NOTE: It is common to place more than 1 OSD on flash devices. To simulate
#       upgrades etc. with this approach this is also used here.
osds_per_device: 1
crush_rule_config: true

##########################
# network

public_network: 192.168.16.0/20
cluster_network: 192.168.16.0/20

# OpenStack pool config moved to inventory/group_vars/ceph.yml

openstack_keys:
  - name: client.cinder-backup
    caps:
      mon: "allow r"
      osd: "allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_cinder_backup_pool.name }}"
    mode: "0600"
  - name: client.cinder
    caps:
      mon: "allow r"
      osd: "allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_cinder_pool.name }}, allow rwx pool={{ openstack_nova_pool.name }}, allow rx pool={{ openstack_glance_pool.name }}"
    mode: "0600"
  - name: client.glance
    caps:
      mon: "allow r"
      osd: "allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_glance_pool.name }}"
    mode: "0600"
  - name: client.gnocchi
    caps:
      mon: "allow r"
      osd: "allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_gnocchi_pool.name }}"
    mode: "0600"
  - name: client.nova
    caps:
      mon: "allow r"
      osd: "allow class-read object_prefix rbd_children, allow rwx pool={{ openstack_glance_pool.name }}, allow rwx pool={{ openstack_nova_pool.name }}, allow rwx pool={{ openstack_cinder_pool.name }}, allow rwx pool={{ openstack_cinder_backup_pool.name }}"
    mode: "0600"

##########################
# dashboard

ceph_dashboard_port: 7000
ceph_dashboard_addr: 192.168.16.10

##########################
# custom

ceph_conf_overrides:
  global:
    auth allow insecure global id reclaim: false
    mon host: 192.168.16.10
    osd crush chooseleaf type: 0
    osd pool default min size: 2
    osd pool default pg num: 8
    osd pool default size: 3

  mon:
    mon allow pool delete: true
    mon max pg per osd: 400

  "client.rgw.{{ hostvars[inventory_hostname]['ansible_hostname'] }}.rgw0":
    "rgw content length compat": "true"
    "rgw enable apis": "swift, s3, admin"
    "rgw keystone accepted roles": "member, admin"
    "rgw keystone accepted admin roles": "admin"
    "rgw keystone admin domain": "default"
    "rgw keystone admin password": "{{ ceph_rgw_keystone_password }}"
    "rgw keystone admin project": "service"
    "rgw keystone admin tenant": "service"
    "rgw keystone admin user": "ceph_rgw"
    "rgw keystone api version": "3"
    "rgw keystone url": "https://api.in-a-box.cloud:5000"
    "rgw keystone implicit tenants": "true"
    "rgw s3 auth use keystone": "true"
    "rgw swift account in url": "true"
    "rgw swift versioning enabled": "true"
    "rgw keystone verify ssl": "false"
    "rgw verify ssl": "false"

# CephFS and RGW pool config moved to inventory/group_vars/ceph.yml
